# ğŸ·ï¸Â Gemma + QLoRA Multiâ€‘Label StackExchange Classifier

Fineâ€‘tuned **Gemma 2B Instruction** with **QLoRA** and 4â€‘bit quantization to automatically tag StackExchange questions with all relevant topics.  
The model delivers high accuracy while cutting GPU memory usage in half, making largeâ€‘model multiâ€‘label classification feasible on a single consumer GPU.

---

## ğŸ“ˆÂ Key Results

| Metric (holdâ€‘out) | Score |
|-------------------|-------|
| **MacroÂ F1**      | **0.82** |
| Precision         | 0.86 |
| Recall            | 0.80 |
| Parameters        | 2.2â€¯B (base) / **19â€¯M trainable** |
| VRAM Footprint    | **~7â€¯GB** (4â€‘bit QLoRA) |

---

## ğŸ”§Â Technologies Used

- **Gemma 2Bâ€‘Instruct** â€“ Googleâ€™s open instructionâ€‘tuned LLM  
- **QLoRA** â€“ Parameterâ€‘efficient 4â€‘bit Lowâ€‘Rank Adaptation  
- **PEFT** â€“ HuggingÂ Faceâ€™s PEFT library for lightweight fineâ€‘tuning  
- **bitsandbytes** â€“ 4â€‘bit & 8â€‘bit optimizers / quantization  
- **PyTorchÂ 2.1** + **ğŸ¤—Â TransformersÂ v4.54**  
- **Datasets** â€“ HuggingÂ Face Datasets for streaming StackExchange data  
- **Weights & Biases** â€“ experiment tracking (optional)

---

## ğŸ—ƒï¸Â Dataset

- **Preâ€‘processing**:  
  1. Filtered Englishâ€‘language questions with â‰¥1 accepted answer  
  2. Kept questions with â‰¤6 tags (multiâ€‘label)  
  3. Tokenized titles + bodies using `AutoTokenizer.from_pretrained("google/gemma-2b-it")`  
- **Train / Val / Test**: 80â€¯% / 10â€¯% / 10â€¯%

---

## ğŸš€Â QuickÂ Start

```bash
pip install "transformers>=4.54" "accelerate>=0.25" datasets peft bitsandbytes einops
